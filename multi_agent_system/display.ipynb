{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42c4ab1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Force reload all modules to clear caches\n",
    "import sys\n",
    "import importlib\n",
    "\n",
    "# Remove all multi_agent_system modules from cache\n",
    "keys_to_delete = [key for key in list(sys.modules.keys()) if 'multi_agent_system' in key or 'langchain' in key or 'langgraph' in key or 'tavily' in key]\n",
    "for key in keys_to_delete:\n",
    "    try:\n",
    "        del sys.modules[key]\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "print(f\"Removed {len(keys_to_delete)} cached modules\")\n",
    "\n",
    "# Also clear importlib caches\n",
    "importlib.invalidate_caches()\n",
    "print(\"Invalidated importlib caches\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e943953",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "# Final cache clear before imports\n",
    "keys_to_delete = [key for key in list(sys.modules.keys()) if 'multi_agent_system' in key]\n",
    "for key in keys_to_delete:\n",
    "    try:\n",
    "        del sys.modules[key]\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "# Ensure we're in the right path\n",
    "sys.path.insert(0, '/Users/lixuepeng03/Jack-Investment-Assistant')\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langgraph.store.memory import InMemoryStore\n",
    "from langgraph_supervisor import create_supervisor\n",
    "\n",
    "from multi_agent_system.config import LLM_CONFIG, SUPERVISOR_PROMPT\n",
    "from multi_agent_system.state import State\n",
    "from multi_agent_system.searcher_agent import create_searcher_agent\n",
    "from multi_agent_system.summary_agent import create_summary_agent\n",
    "from multi_agent_system.utils import show_graph\n",
    "\n",
    "\"\"\"\n",
    "Create and compile the supervisor agent.\n",
    "    \n",
    "Returns:\n",
    "    langgraph.graph: Compiled supervisor agent graph.\n",
    "\"\"\"\n",
    "# Initialize the LLM\n",
    "llm = ChatOpenAI(**LLM_CONFIG)\n",
    "    \n",
    "# Initialize memory components\n",
    "checkpointer = MemorySaver()\n",
    "in_memory_store = InMemoryStore()\n",
    "\n",
    "# Create sub-agents\n",
    "searcher_agent = create_searcher_agent()\n",
    "summary_agent = create_summary_agent()\n",
    "# coder_agent = create_coder_agent()\n",
    "    \n",
    "\n",
    "# Import the supervisor creator to avoid name collision\n",
    "from langgraph_supervisor import create_supervisor as create_supervisor_workflow\n",
    "\n",
    "supervisor_prebuilt_workflow = create_supervisor_workflow(\n",
    "        agents=[searcher_agent, summary_agent],\n",
    "        output_mode=\"last_message\",\n",
    "        model=llm,\n",
    "        prompt=SUPERVISOR_PROMPT,\n",
    "        state_schema=State\n",
    "    )\n",
    "        \n",
    "supervisor_prebuilt = supervisor_prebuilt_workflow.compile(\n",
    "        name=\"supervisor\",\n",
    "        checkpointer=checkpointer,\n",
    "        store=in_memory_store\n",
    "    )\n",
    "    \n",
    "show_graph(supervisor_prebuilt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9132d7c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "# Final cache clear before imports\n",
    "keys_to_delete = [key for key in list(sys.modules.keys()) if 'multi_agent_system' in key]\n",
    "for key in keys_to_delete:\n",
    "    try:\n",
    "        del sys.modules[key]\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "# Ensure we're in the right path\n",
    "sys.path.insert(0, '/Users/lixuepeng03/Jack-Investment-Assistant')\n",
    "\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langgraph.store.memory import InMemoryStore\n",
    "from langgraph_supervisor import create_supervisor\n",
    "\n",
    "from multi_agent_system.config import LLM_CONFIG, SUPERVISOR_PROMPT\n",
    "from multi_agent_system.state import State\n",
    "from multi_agent_system.searcher_agent import create_searcher_agent\n",
    "from multi_agent_system.summary_agent import create_summary_agent\n",
    "from multi_agent_system.utils import show_graph\n",
    "\n",
    "\n",
    "def create_supervisor():\n",
    "    \"\"\"\n",
    "    Create and compile the supervisor agent.\n",
    "    \n",
    "    Returns:\n",
    "        langgraph.graph: Compiled supervisor agent graph.\n",
    "    \"\"\"\n",
    "    # Initialize the LLM\n",
    "    llm = ChatOpenAI(**LLM_CONFIG)\n",
    "    \n",
    "    # Initialize memory components\n",
    "    checkpointer = MemorySaver()\n",
    "    in_memory_store = InMemoryStore()\n",
    "\n",
    "    # Create sub-agents\n",
    "    searcher_agent = create_searcher_agent()\n",
    "    summary_agent = create_summary_agent()\n",
    "    # coder_agent = create_coder_agent()\n",
    "    \n",
    "\n",
    "    # Import the supervisor creator to avoid name collision\n",
    "    from langgraph_supervisor import create_supervisor as create_supervisor_workflow\n",
    "\n",
    "    supervisor_prebuilt_workflow = create_supervisor_workflow(\n",
    "            agents=[searcher_agent, summary_agent],\n",
    "            output_mode=\"last_message\",\n",
    "            model=llm,\n",
    "            prompt=SUPERVISOR_PROMPT,\n",
    "            state_schema=State\n",
    "        )\n",
    "        \n",
    "    supervisor_prebuilt = supervisor_prebuilt_workflow.compile(\n",
    "            name=\"supervisor\",\n",
    "            checkpointer=checkpointer,\n",
    "            store=in_memory_store\n",
    "        )\n",
    "    \n",
    "    show_graph(supervisor_prebuilt)\n",
    "    \n",
    "    return supervisor_prebuilt\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \"\"\"Test the supervisor independently.\"\"\"\n",
    "    import uuid\n",
    "    from langchain_core.messages import HumanMessage\n",
    "    \n",
    "    # Create the supervisor\n",
    "    supervisor = create_supervisor()\n",
    "    \n",
    "    thread_id = uuid.uuid4()\n",
    "    question = \"\"\n",
    "    config = {\"configurable\": {\"thread_id\": thread_id}}\n",
    "    \n",
    "    print(f\"Thread ID: {thread_id}\")\n",
    "    print(f\"Question: {question}\\n\")\n",
    "    \n",
    "    result = supervisor.invoke(\n",
    "        {\"messages\": [HumanMessage(content=question)]},\n",
    "        config=config\n",
    "    )\n",
    "    \n",
    "    print(\"Response:\")\n",
    "    for message in result[\"messages\"]:\n",
    "        message.pretty_print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5e30e34",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Summary agent for web search and information retrieval.\"\"\"\n",
    "import sys\n",
    "\n",
    "# Final cache clear before imports\n",
    "keys_to_delete = [key for key in list(sys.modules.keys()) if 'multi_agent_system' in key]\n",
    "for key in keys_to_delete:\n",
    "    try:\n",
    "        del sys.modules[key]\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "# Ensure we're in the right path\n",
    "sys.path.insert(0, '/Users/lixuepeng03/Investment-Assistant')\n",
    "\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langgraph.store.memory import InMemoryStore\n",
    "from multi_agent_system.state import State\n",
    "from multi_agent_system.utils import show_graph\n",
    "from multi_agent_system.memory_tool import load_memory\n",
    "from multi_agent_system.supervisor import create_supervisor\n",
    "from multi_agent_system.memory_agent import create_memory\n",
    "from langgraph.checkpoint.memory import MemorySaver # For short-term memory (thread-level state persistence)\n",
    "from langgraph.store.memory import InMemoryStore # For long-term memory (storing user preferences)\n",
    "from multi_agent_system.config import LLM_CONFIG\n",
    "from langgraph.store.base import BaseStore # Base class for defining custom stores for LangGraph\n",
    "\n",
    "llm = ChatOpenAI(**LLM_CONFIG)\n",
    "\n",
    "in_memory_store = InMemoryStore()\n",
    "\n",
    "checkpointer = MemorySaver()\n",
    "\n",
    "invest_assistan_agent = StateGraph(State)\n",
    "\n",
    "invest_assistan_agent.add_node(\"load_memory\", load_memory) \n",
    "invest_assistan_agent.add_node(\"supervisor\", create_supervisor)  # Call the function to get the compiled graph\n",
    "invest_assistan_agent.add_node(\"create_memory\", create_memory)\n",
    "\n",
    "invest_assistan_agent.add_edge(START, \"load_memory\")\n",
    "invest_assistan_agent.add_edge(\"load_memory\", \"supervisor\")\n",
    "invest_assistan_agent.add_edge(\"supervisor\", \"create_memory\")\n",
    "invest_assistan_agent.add_edge(\"create_memory\", END)\n",
    "\n",
    "# Compile the entire, sophisticated graph.\n",
    "invest_assistant_workflow = invest_assistan_agent.compile(name=\"multi_agent_verify\", checkpointer=checkpointer, store=in_memory_store)\n",
    "\n",
    "show_graph(invest_assistant_workflow)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25702089",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Summary agent for web search and information retrieval.\"\"\"\n",
    "import sys\n",
    "\n",
    "# Final cache clear before imports\n",
    "keys_to_delete = [key for key in list(sys.modules.keys()) if 'multi_agent_system' in key]\n",
    "for key in keys_to_delete:\n",
    "    try:\n",
    "        del sys.modules[key]\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "# Ensure we're in the right path\n",
    "sys.path.insert(0, '/Users/lixuepeng03/Investment-Assistant')\n",
    "\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langgraph.store.memory import InMemoryStore\n",
    "from multi_agent_system.state import State\n",
    "from multi_agent_system.utils import show_graph\n",
    "from multi_agent_system.memory_tool import load_memory\n",
    "from multi_agent_system.supervisor import create_supervisor\n",
    "from multi_agent_system.memory_agent import create_memory\n",
    "from langgraph.checkpoint.memory import MemorySaver # For short-term memory (thread-level state persistence)\n",
    "from langgraph.store.memory import InMemoryStore # For long-term memory (storing user preferences)\n",
    "from multi_agent_system.config import LLM_CONFIG\n",
    "from langgraph.store.base import BaseStore # Base class for defining custom stores for LangGraph\n",
    "\n",
    "\n",
    "\n",
    "def create_invest_assistant():\n",
    "    \"\"\"Creates a multi-agent investment assistant with memory management.\"\"\"\n",
    "     # Initialize the LLM\n",
    "    llm = ChatOpenAI(**LLM_CONFIG)\n",
    "\n",
    "    in_memory_store = InMemoryStore()\n",
    "\n",
    "    checkpointer = MemorySaver()\n",
    "\n",
    "    invest_assistan_agent = StateGraph(State)\n",
    "\n",
    "    invest_assistan_agent.add_node(\"load_memory\", load_memory) \n",
    "    invest_assistan_agent.add_node(\"supervisor\", create_supervisor)\n",
    "    invest_assistan_agent.add_node(\"create_memory\", create_memory)\n",
    "\n",
    "    invest_assistan_agent.add_edge(START, \"load_memory\")\n",
    "    invest_assistan_agent.add_edge(\"load_memory\", \"supervisor\")\n",
    "    invest_assistan_agent.add_edge(\"supervisor\", \"create_memory\")\n",
    "    invest_assistan_agent.add_edge(\"create_memory\", END)\n",
    "\n",
    "    # Compile the entire, sophisticated graph.\n",
    "    invest_assistant_workflow = invest_assistan_agent.compile(name=\"multi_agent_verify\", checkpointer=checkpointer, store=in_memory_store)\n",
    "    \n",
    "    return invest_assistant_workflow\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \"\"\"Test the invest assistant independently.\"\"\"\n",
    "    import uuid\n",
    "    from langchain_core.messages import HumanMessage\n",
    "    \n",
    "    # Create the invest assistant\n",
    "    invest_assistant = create_invest_assistant()\n",
    "\n",
    "    show_graph(invest_assistant)\n",
    "\n",
    "    thread_id = uuid.uuid4()\n",
    "    config = {\"configurable\": {\"thread_id\": thread_id, \"user_id\" : \"5\"}}\n",
    "\n",
    "    question = \"\"\n",
    "\n",
    "    result = invest_assistant.invoke({\"messages\": [HumanMessage(content=question)]}, config=config)\n",
    "\n",
    "    # Print results\n",
    "    print(\"Response:\")\n",
    "    result[\"messages\"][-1].pretty_print()\n",
    "    # for message in result[\"messages\"]:\n",
    "    #     message.pretty_print() \n",
    "\n",
    "\n",
    "    print(\"---------分割线----------\\n\")\n",
    "\n",
    "    \n",
    "\n",
    "    question2 = \"\"\n",
    "    result2 = invest_assistant.invoke({\"messages\": [HumanMessage(content=question2)]}, config=config)\n",
    "    \n",
    "\n",
    "    # Print results\n",
    "    print(\"Response:\")\n",
    "    result2[\"messages\"][-1].pretty_print()\n",
    "    # for message in result2[\"messages\"]:\n",
    "    #     message.pretty_print() \n",
    "    \n",
    "    \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.13.1)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
